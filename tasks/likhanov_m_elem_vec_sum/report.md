# Сумма элементов вектора

- Студент: Лиханов Матвей Дмитриевич, группа 3823Б1ПР1  
- Технология: MPI + SEQ
- Вариант: 1

## 1. Введение

Цель данной работы — разработка алгоритма вычисления суммы
элементов вектора, реализованного как **последовательно (SEQ)**,
так и с использованием **параллельной технологии MPI**.  
Задача демонстрирует основные подходы к распараллеливанию
вычислительно интенсивных операций на многопроцессорных системах.

## 2. Постановка задачи

**Определение:**  

- Необходимо вычислить сумму элементов вектора размера N,
содержащего целые числа от 1 до N.

**Ограничения:**  

- Входные данные — вектор целых чисел произвольной длины.
- Параллельная реализация должна корректно работать с
любым размером вектора.
- Используется модель передачи сообщений (MPI).
- Результаты обеих реализаций (SEQ и MPI) должны совпадать.

## 3. Описание алгоритма (базового/последовательного)

Последовательная версия алгоритма просуммировывает все
элементы вектора в цикле от 1 до N.

**Сложность:** O(n), где n — размер вектора.

### Код последовательной реализации

```cpp
bool LikhanovMElemVecSumSEQ::RunImpl() {
  const int64_t n = GetInput();

  int64_t sum = 0;
  for (int64_t i = 1; i <= n; ++i) {
    sum += i;
  }

  GetOutput() = sum;
  return true;
}
```

## 4. Схема распараллеливания

В MPI-версии алгоритм разбивает диапазон [1..N] между
всеми процессами. Каждый процесс вычисляет локальную сумму,
после чего используется операция MPI_Reduce для получения
глобальной суммы на процессе rank == 0.

### Основные этапы алгоритма

1. Получение номера процесса (rank) и количества процессов (size).
2. Разделение диапазона [1..N] между процессами с учётом остатка
N % size.
3. Вычисление локальной суммы на каждом процессе.
4. Сбор всех локальных сумм с помощью MPI_Reduce на root-процессе.
5. Сохранение глобальной суммы в GetOutput() на root-процессе.

Данная схема распараллеливания использует разбиение данных по
диапазонам индексов и коллективную редукцию, что является типичным
подходом для задач редукционного типа.

### Ключевой фрагмент MPI-реализации

```cpp
bool LikhanovMElemVecSumMPI::RunImpl() {
  int rank = 0, size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  const int64_t n = GetInput();

  int64_t base = n / size;
  int64_t rem = n % size;

  int64_t local_begin = rank * base + std::min<int64_t>(rank, rem) + 1;

  int64_t local_end = local_begin + base - 1 + (rank < rem ? 1 : 0);

  int64_t local_sum = 0;
  for (int64_t i = local_begin; i <= local_end; ++i) {
    local_sum += i;
  }

  int64_t global_sum = 0;
  MPI_Reduce(
    &local_sum,
    &global_sum,
    1, MPI_INT64_T,
    MPI_SUM, 0,
    MPI_COMM_WORLD
  );

  if (rank == 0) {
    GetOutput() = global_sum;
  }

  return true;
}
```

## 5. Экспериментальные результаты

### Оценка производительности

- Аппаратное обеспечение и операционная система
  - Процессор: Apple M3
  - Оперативная память: 8 ГБ
  - Хост-операционная система: macOS Tahoe 26.2
- Инструменты
  - Среда разработки: Visual Studio Code
  - Окружение выполнения: Docker-контейнер
  - Гостевая ОС контейнера: Ubuntu Linux
  - Компилятор: GCC (используемый по умолчанию в контейнере)
  - Тип сборки: Release
  - MPI: Open MPI

Использовался вектор размером 100 000 000 элементов.

#### Время выполнения `task_run`

| Реализация | Кол-во процессов | Время выполнения (с) | Ускорение |
|------------|----------------- |--------------------  |-----------|
| SEQ        | 1                | 0.0256               | 1.00      |
| MPI        | 2                | 0.0134               | 1.91      |
| MPI        | 4                | 0.0073               | 3.51      |
| MPI        | 8                | 0.0065               | 3.94      |

#### Вывод по результатам

- Ускорение достигается за счёт распараллеливания работы по процессам.
- Каждый процесс вычисляет локальную часть суммы, что уменьшает количество
операций на одном процессе.
- При увеличении числа процессов ускорение растёт, но не линейно, так как
появляются накладные расходы на коммуникацию (MPI_Reduce) и распределение
диапазона.
- На 8 процессах ускорение ≈ 3.94× вместо идеальных 8×, что объясняется
коммуникационными затратами и остатком элементов при делении диапазона.

## 6. Заключение

### Корректность

- SEQ и MPI версии вычисляют одинаковую сумму для всех тестовых случаев.
- Проверка с использованием функциональных тестов (gtest) подтверждает
корректность.

### Производительность

- MPI-реализация демонстрирует существенное ускорение по сравнению с
последовательной версией при увеличении числа процессов.
- Последовательная версия линейно масштабируется по N.

### Масштабируемость

- Алгоритм хорошо масштабируется при увеличении числа процессов (P),
особенно для больших N.
- Основное ограничение — накладные расходы на коммуникацию при больших P
и маленьком N.
