# Гиперкуб

- Студент: Лиханов Матвей Дмитриевич, группа 3823Б1ПР1  
- Технология: MPI + SEQ
- Вариант: 10

## 1. Введение

В данной работе реализованы алгоритмы подсчёта рёбер n-мерного гиперкуба.
Рассматриваются две реализации: последовательная (SEQ) и параллельная с
использованием MPI (MPI). Основная цель — изучение подходов к
распараллеливанию задач на виртуальной топологии гиперкуба и сравнение
их производительности.

## 2. Постановка задачи

**Цель работы:**

- Разработать алгоритм подсчёта количества рёбер n-мерного гиперкуба.
- Реализовать последовательную версию для проверки корректности.
- Реализовать параллельную версию с использованием MPI, с передачей данных
через соседей виртуальной гиперкубной топологии.
- Обеспечить корректную синхронизацию процессов и отсутствие deadlock.
- Провести экспериментальное сравнение производительности SEQ и MPI версий.

**Требования к MPI версии:**

- Использовать виртуальную гиперкубную топологию (через XOR соседей).
- Не использовать `MPI_Cart_create` или `MPI_Graph_create`.
- Обеспечить возможность передачи данных от любого процесса к любому другому
через соседей.

## 3. Описание алгоритма (базового/последовательного)

Алгоритм перебирает все вершины гиперкуба и для каждой вершины считает рёбра,
используя XOR для определения соседей. Для того чтобы каждое ребро учитывалось
один раз, используется проверка `v < neighbor`.

**Пример реализации:**

```cpp
bool LikhanovMHypercubeSEQ::RunImpl() {
  const InType dimension = GetInput();

  const std::uint64_t vertices = static_cast<std::uint64_t>(1) << dimension;
  std::uint64_t total_edges = 0;

  for (std::uint64_t v = 0; v < vertices; ++v) {
    for (InType bit = 0; bit < dimension; ++bit) {
      const std::uint64_t neighbor = v ^ (1ULL << bit);
      if (v < neighbor) {
        ++total_edges;
      }
    }
  }

  GetOutput() = static_cast<OutType>(total_edges);
  return true;
}
```

## 4. Схема распараллеливания

Параллельный алгоритм разделяет вершины гиперкуба между процессами,
каждый процесс считает локальное количество рёбер, затем выполняется
hypercube reduction к процессу rank 0 через соседей виртуальной топологии.

**Пример реализации:**

```cpp
bool LikhanovMHypercubeMPI::RunImpl() {
  int rank = 0, size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  if (!(size > 0 && ((size & (size - 1)) == 0))) return false;

  const int dimension = static_cast<int>(std::log2(size));
  const InType n = GetInput();
  const std::uint64_t vertices = 1ULL << n;

  const std::uint64_t u_rank = static_cast<std::uint64_t>(rank);
  const std::uint64_t chunk = vertices / size;
  const std::uint64_t remainder = vertices % size;

  const std::uint64_t start = u_rank * chunk + (
    u_rank < remainder ? u_rank : remainder);
  const std::uint64_t local_size = chunk + (
    u_rank < remainder ? 1ULL : 0ULL);
  const std::uint64_t end = start + local_size;

  std::uint64_t local_edges = 0;
  for (std::uint64_t v = start; v < end; ++v) {
    for (InType bit = 0; bit < n; ++bit) {
      const std::uint64_t neighbor = v ^ (1ULL << bit);
      if (v < neighbor) ++local_edges;
    }
  }

  std::uint64_t sum = local_edges;
  for (int k = 0; k < dimension; ++k) {
    int partner = rank ^ (1 << k);
    if ((rank & (1 << k)) != 0) {
      MPI_Send(&sum, 1, MPI_UINT64_T, partner, 0, MPI_COMM_WORLD);
      break;
    } else {
      if (partner < size) {
        std::uint64_t received = 0;
        MPI_Recv(
            &received, 1, MPI_UINT64_T, 
            partner, 0, MPI_COMM_WORLD, 
            MPI_STATUS_IGNORE);
        sum += received;
      }
    }
  }

  MPI_Bcast(&sum, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);
  GetOutput() = static_cast<OutType>(sum);

  return true;
}
```

**Пояснение:**

- `rank ^ (1 << k)` — сосед по гиперкубной виртуальной топологии.
- Каждый процесс либо отправляет данные и завершает участие,
либо принимает и суммирует.
- В конце результат рассылается всем процессам через `MPI_Bcast`.
- Сложность редукции: O(log₂(P)), где P — число процессов.

## 5. Экспериментальные результаты

### Оценка производительности

- Аппаратное обеспечение и операционная система
  - Процессор: Apple M3
  - Оперативная память: 8 ГБ
  - Хост-операционная система: macOS Tahoe 26.2
- Инструменты
  - Среда разработки: Visual Studio Code
  - Окружение выполнения: Docker-контейнер
  - Гостевая ОС контейнера: Ubuntu Linux
  - Компилятор: GCC (используемый по умолчанию в контейнере)
  - Тип сборки: Release
  - MPI: Open MPI

### Время выполнения `task_run`

| Реализация | Кол-во процессов | Время выполнения (с) | Ускорение |
|------------|----------------- |--------------------  |-----------|
| SEQ        | 1                | 0.242                |   1.00    |
| MPI        | 2                | 0.125                |   1.94    |
| MPI        | 4                | 0.069                |   3.51    |
| MPI        | 8                | 0.067                |   3.61    |

#### Вывод по результатам

- MPI-реализация показывает почти линейное ускорение при переходе от
1 к 4 процессам.
- При 8 процессах ускорение почти не растёт (3.61 против 3.51 при 4 процессах),
что объясняется накладными расходами на коммуникацию и низкой вычислительной
нагрузкой на процесс при большом числе процессов.
- Алгоритм эффективен для умеренного числа процессов, при котором локальная
работа на каждом процессе достаточно велика, чтобы перекрыть затраты
на обмен данными.
- Параллельная реализация через виртуальную гиперкубную топологию корректна
и даёт существенный выигрыш по времени по сравнению с последовательной реализацией.

## 6. Заключение

- Реализованы последовательная и параллельная версии подсчёта рёбер гиперкуба.
- MPI-версия использует виртуальную топологию гиперкуба через XOR соседей,
не использует MPI_Cart_create или MPI_Graph_create.
- Корректность проверена с помощью perf-тестов; результаты совпадают с
формулой n * 2^(n-1).
- Распараллеливание позволяет эффективно уменьшить время вычисления при
увеличении числа процессов.
